{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/strnad/PIK/pyDRLinWESM\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'plots.reward_progress_plots' from '/home/strnad/PIK/pyDRLinWESM/plots/reward_progress_plots.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "from importlib import reload\n",
    "%cd ../\n",
    "\n",
    "import DeepReinforcementLearning.DQNLearner as drl\n",
    "import plots.reward_progress_plots as rpp\n",
    "reload(drl)\n",
    "reload(rpp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the environment you want to use \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we decide to use the AYS environment. \n",
    "# This environment requires the pyviability package of Tim Kittel (https://timkittel.github.io/PyViability/)\n",
    "import AYS.AYS_Environment as ays\n",
    "reload(ays)\n",
    "    \n",
    "dt=1\n",
    "max_steps=600 # in environment otherwise fix point is reached\n",
    "reward_type='PB'# might be as well 'survive'\n",
    "\n",
    "learning_analysis_folder='AYS'\n",
    "observables=dict(A=True,\n",
    "                 Y=True,\n",
    "                 S=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up simulation for learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes=10000  # total number of learning episodes\n",
    "episode_steps=100  # analysis steps of learning behavior\n",
    "noise_strength=0.00\n",
    "\n",
    "# DRL Hyperparameters\n",
    "batch_size = 64     # Number of experiences the Batch replay can keep\n",
    "network_learning_rate= 0.00025\n",
    "gamma=0.96\n",
    "Boltzmann_prob=False\n",
    "memory_size = int(1e5)  # for larger environments might be nessesary to be set larger \n",
    "\n",
    "# Specifiy the DRL learner\n",
    "prior_exp_replay=True\n",
    "importance_sampling=True\n",
    "noisy_net=False\n",
    "learner_type='ddqn'# might be also 'dqn','fdqn','c51'\n",
    "dueling_networks=True\n",
    "\n",
    "# parameters for the agent\n",
    "Update_target_frequency=100\n",
    "explore_start = 1.0 \n",
    "explore_stop = 0.01 \n",
    "decay_rate=0.001 \n",
    "drl_learner=learner_type\n",
    "if prior_exp_replay:\n",
    "    drl_learner+='_per'\n",
    "if importance_sampling:\n",
    "    drl_learner+='_is'\n",
    "if dueling_networks:\n",
    "    drl_learner+='_duel'        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init AYS Environment! \n",
      "Reward Type: PB \n",
      "Sustainability Boundaries [A_PB, Y_SF, S_ren]:  [3.45e+02 4.00e+13 0.00e+00]\n",
      "current directory is : ./AYS/\n",
      "Learner: Store results in:./AYS//ddqn_per_is_duel/epsilon_greedy/PB\n",
      "Using the following environment: <class 'AYS.AYS_Environment.AYS_Environment'>\n",
      "Created Dueling Networks!\n"
     ]
    }
   ],
   "source": [
    "reload(drl)\n",
    "reload(ays)\n",
    "full_observables=dict( A=True, Y=True, S=True  )\n",
    "my_Env=ays.AYS_Environment(dt=dt, reward_type=reward_type)\n",
    "dirpath='./AYS/'\n",
    "plot_progress=True\n",
    "\n",
    "dqn_agent=drl.DQNLearner(my_Env=my_Env, dt=dt,  episodes=episode_steps, max_steps=max_steps, \n",
    "                         gamma=gamma, explore_start = explore_start  , explore_stop = explore_stop,  decay_rate=decay_rate , Boltzmann_prob=Boltzmann_prob,\n",
    "                         reward_type=reward_type, Update_target_frequency=Update_target_frequency,\n",
    "                         learner_type=learner_type, prior_exp_replay=prior_exp_replay, dueling=dueling_networks, importance_sampling=importance_sampling, \n",
    "                         noisy_net=noisy_net, memory_size = memory_size, batch_size = batch_size , network_learning_rate=network_learning_rate, \n",
    "                         plot_progress=plot_progress, dirpath=dirpath, learning_analysis_folder='AYS')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory is initialized!\n",
      "episode:  0\n"
     ]
    }
   ],
   "source": [
    "results_path=dqn_agent.results_path\n",
    "num_runs= int(episodes/episode_steps) + 1\n",
    "\n",
    "# Reset and initialize memory of agent\n",
    "dqn_agent.reset_learner(run_number=0)\n",
    "    \n",
    "for run in range(num_runs ):\n",
    "    # Here we train the agent with runs times episodes, we do this after evaluation to get the performance of the untrained agent as well.\n",
    "    dqn_agent.learn()    \n",
    "    \n",
    "    # Visualize learning results\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    \n",
    "    # Test the agents behavior \n",
    "    result=dqn_agent.test_on_current_state(save_plot=True, show_plot=True)  # save_plot=True if we Trajectory of test should be stored!\n",
    "    rpp.plot_one_learning_developement(path='./', model='AYS',learner_type=drl_learner, reward_type=reward_type,label=drl_learner )\n",
    "    \n",
    "    \n",
    "# Evaluate Final results:\n",
    "print(dqn_agent.test_on_current_state(save_plot=True, show_plot=True))\n",
    "\n",
    "print(\"Episodes Learning: Stored results into \" + results_path)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
